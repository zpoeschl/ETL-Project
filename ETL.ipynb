{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from config import username, password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diversity Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we loaded our most complex dataset: the diversity index CSV.\n",
    "\n",
    "To transform the data, we first split the location column into separate county and state columns to allow interaction with the other two datasets.\n",
    "\n",
    "State averages were also included in the dataset and listed as a full state name in the location column. Since we only wanted data by county, once locations were split into county and state, we dropped all rows that had state only info.\n",
    "    For example, most locations followed the “county, state abbreviation” format (Alameda County, CA), however state averages were listed by just the state name in all caps (TEXAS). Once counties and states were split into separate columns, those rows with only a state average resulted in a value of ‘NaN’ in column our new State column. We were then able to drop all the state average rows by dropping all rows with a State value of ‘NaN’.\n",
    "\n",
    "We verified this was successful by checking that no results were found when searching our new County column by a full state name, in this case ‘TEXAS’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_csv = \"Resources/diversityindex.csv\"\n",
    "diversity_df = pd.read_csv(diversity_csv)\n",
    "diversity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a a new df with info we need \n",
    "\n",
    "# Split the County coloum into State and County\n",
    "new_df = diversity_df\n",
    "new_df['County'], new_df['State'] = diversity_df['Location'].str.split(',').str\n",
    "new_df.tail()\n",
    "\n",
    "# Drops all rows that have NaN as County as States got split into County above. \n",
    "new_df = new_df.dropna()\n",
    "\n",
    "#checking if this worked and drop was sucessful in removing STATe names from county coloum \n",
    "\n",
    "# dsd = df[df['County'] == 'TEXAS']\n",
    "# dsd.tail()\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop uneccessary column \"Location\" - replaced by county, state columns\n",
    "diversity_df = new_df.drop(['Location'], axis=1)\n",
    "\n",
    "# rename columns for compatibility with SQL\n",
    "diversity_df = diversity_df.rename(columns={'Diversity-Index': 'diversity_index',\n",
    "                                            'Black or African American alone, percent, 2013': 'black',\n",
    "                                           'American Indian and Alaska Native alone, percent, 2013': 'native_am',\n",
    "                                           'Asian alone, percent, 2013': 'asian',\n",
    "                                           'Native Hawaiian and Other Pacific Islander alone, percent,': 'hawaiian',\n",
    "                                           'Two or More Races, percent, 2013': 'mixed',\n",
    "                                           'Hispanic or Latino, percent, 2013': 'latinx',\n",
    "                                           'White alone, not Hispanic or Latino, percent, 2013': 'white',\n",
    "                                           'County': 'county',\n",
    "                                           'State': 'state'})\n",
    "\n",
    "diversity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unemployment Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then loaded the unemployment dataset. We discovered that this dataset was organized quite differently from the others in that it was broken down first by year, then by month, then by state and county. This meant we had unemployment data broken out by month from 1990 up to 2016, resulting in a far larger dataset than our other two CSVs. To deal with this discrepancy, we averaged the unemployment rate by grouping across county and state.\n",
    "\n",
    "For this dataset, states were also not in a format which was compatible with our other two datasets. To fix this, we located “chocolate cake recipe” code for quickly converting full state names into state abbreviations and applied this to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_csv = \"Resources/unemployment.csv\"\n",
    "unemp_df = pd.read_csv(unemp_csv)\n",
    "\n",
    "unemp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_df = unemp_df[['County', 'State', 'Rate']]\n",
    "unemp_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_state_abbrev = {\n",
    "    \n",
    "'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA', 'Colorado': 'CO',\n",
    "'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID',\n",
    "'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA',\n",
    "'Maine': 'ME', 'Maryland': 'MD', 'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
    "'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ',\n",
    "'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n",
    "'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD',\n",
    "'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA',\n",
    "'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_df['State'] = unemp_df['State'].map(us_state_abbrev).fillna(unemp_df['State'])\n",
    "\n",
    "# rename columns for compatibility with SQL\n",
    "unemp_df = unemp_df.rename(columns={'County': 'county', 'State': 'state', 'Rate': 'rate'})\n",
    "\n",
    "# drop duplicate rows\n",
    "unemp_df = unemp_df.drop_duplicates(subset=['county', 'state'])\n",
    "\n",
    "unemp_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Median Income Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we loaded in our median income dataset. This was our cleanest dataset and required very little transformation on our part to be compatible with the other two. County and state abbreviations were already in place and broken out into separate columns. Median household income was also already in a usable format. The only necessary cleanup was to drop columns that contained either redundant info (County-State and State (full name) or info not needed (Population)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "median_csv = \"Resources/medianincome.csv\"\n",
    "median_df = pd.read_csv(median_csv)\n",
    "median_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_df = median_df[['County',\"State Code\", 'Population',\"Median household income\"]]\n",
    "\n",
    "# rename columns for compatibility with SQL\n",
    "median_df = median_df.rename(columns={'County': 'county', 'State Code': 'state', 'Population': 'population', 'Median household income': 'median_household_income'})\n",
    "\n",
    "median_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load to SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all three datasets were extracted and transformed, we loaded them into three tables within an SQL database. For future hypothetical analysis, data across the three tables could be compared using the composite keys for each table of county and state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "rds_connection_string = f\"{username}:{password}@localhost:5432/ETL_db\"\n",
    "engine = create_engine(f'postgresql://{rds_connection_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for tables\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to load converted dataframes into ETL_db\n",
    "median_df.to_sql(name='income', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_df.to_sql(name='diversity', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_df.to_sql(name='unemployment', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirm load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('select * from income', con=engine).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('select * from diversity', con=engine).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('select * from unemployment', con=engine).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
